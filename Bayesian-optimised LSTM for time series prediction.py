# -*- coding: utf-8 -*-
"""FINAL CODE

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kD8F1zgJHs7TH94nGdGH-LWRQAc0lvqk
"""

#intalling all packages for text preprocessing
!pip install transformers
!pip install demoji
!pip install emoji
!pip install tweet-preprocessor
!pip install text-preprocessing
!pip install emot
!pip3 install googletrans==3.1.0a0

# importing all the packages for text preprocessing

import emoji
import demoji
import re
import preprocessor as p
from text_preprocessing import preprocess_text
from text_preprocessing import check_spelling, remove_whitespace, to_lower, remove_punctuation
demoji.download_codes()
import pandas as pd
import numpy as np

# importing the file with the raw Apple tweets

import pandas as pd

df = pd.read_csv('500K.csv', engine ='python')

# keeping only the column text containing the tweets and the date of each tweets

df = df.filter(['Date', 'Tweet'])
df['Tweet'].dropna()
df = df[0:15]

df

# converting slang words into traditional words by accessing www.noslang.com

import requests
prefixStr = '<div class="translation-text">'
postfixStr = '</div'
def clean_5(text):
    r = requests.post('https://www.noslang.com/', {'action': 'translate', 'p': text, 'noswear': 'noswear', 'submit': 'Translate'})
    startIndex = r.text.find(prefixStr)+len(prefixStr)
    endIndex = startIndex + r.text[startIndex:].find(postfixStr)
    return r.text[startIndex:endIndex]

df['Tweet'] = df['Tweet'].apply(clean_5)

df

# translating non-english tweets into english ones

from googletrans import Translator

translator = Translator()

df['Tweet'] = df['Tweet'].apply(translator.translate, dest='en').apply(getattr, args=('text',))

#translated_text = translator.translate('안녕하세요.')
#print(translated_text.text)
df

def cleaner(tweet):
    tweet = re.sub("@[A-Za-z0-9]+","",str(tweet)) #Remove @
    tweet = emoji.demojize(tweet) #Turn emojies into tags
    #tweet = re.sub(r'[^\w\s]|(.)(?=\1\1)', '', tweet)
    tweet = re.sub('a{3,}', 'laugh', tweet) # 3 'a' into 'laugh'
    tweet = re.sub('A{3,}', 'laugh', tweet)# 3 'A' into 'laugh'
    return tweet

df['Tweet'] = df['Tweet'].map(lambda x: cleaner(x))

df

# getting a list of all tweets

headlines_array = np.array(df["Tweet"])

headlines_list = list(headlines_array[:])

cleanedList = [x for x in headlines_list if str(x) != 'nan']

all(isinstance(item, str) for item in cleanedList) #verifying all tweets are strings

len(cleanedList)

# accessing the pre-trained finBERT model and sentiment analysis pipline

from transformers import BertTokenizer, BertForSequenceClassification
from transformers import pipeline

finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)

tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')
nlp = pipeline("sentiment-analysis", model=finbert, tokenizer=tokenizer)

# performing sentiment analysis
try:
    results = nlp(cleanedList)
except Exception:
    pass

print(results)

df_sentiment = pd.read_csv('Final_csv.csv') #time-series file has to be in the 'content' folder if used with Google Colab

df_sentiment = df_sentiment.join(df['Date'])

from datetime import datetime

df_sentiment['Date'] = pd.to_datetime(df_sentiment['Date'])
df_sentiment['Date'] = df_sentiment['Date'].dt.date

df_sentiment.groupby('Date')['Date'].count()

df_sentiment = df_sentiment[df_sentiment['label'].str.contains('Neutral')==False]

df_sentiment

df_sentiment = df_sentiment.drop(df['Unnamed: 0'])

# computing the weights for each day

l = []

for x in df_sentiment.groupby('Date')['Date'].count():
    x = x/14014*143
    l.append(x)
    df_weight = pd.DataFrame({'weight':l})

df_weight

df_weight.to_csv('weights')

df_sentiment = df_sentiment[df_sentiment['label'].str.contains('Neutral')==False]

df_final = df_sentiment.groupby(['Date','label'])['label'].count().unstack(fill_value=0)

df_final = df_final.div(df_final.sum(axis=1), axis=0)

df_final

df_final.reset_index(inplace=True)

df_final.index = range(0,len(df_final))

df_final = df_final.join(df_weight['weight'])

df_final_weighted = df_final.set_index('Date')

df_final_weighted.to_csv('Weights and Emotions.csv')

df_final_weighted

df_final_weighted['Weighted_Postitive'] = df_final_weighted['Positive']*df_final_weighted['weight']
df_final_weighted['Weighted_Negative'] = df_final_weighted['Negative']*df_final_weighted['weight']

df_final_weighted = df_final_weighted.drop(['Positive', 'Negative', 'weight'], axis=1)

df_final_weighted

"""Loading Apple historical time-series using pandas library

"""

import pandas as pd

df_ts = pd.read_csv('AAPL.csv') #time-series file has to be in the 'content' folder if used with Google Colab
df_ts = df_ts[['Date', 'Adj Close']]

import pandas as pd

df_1 = pd.read_csv('AAPL.csv')

df_1

df_2 = pd.read_csv('TSLA.csv')

df_2

df_2_index = df_2.set_index('Date')

df_2_index

df_1_index = df_1.set_index('Date')

df_1

df_1['Open']

df_1_index

df_3 = df_1_index.join(df_2_index)

df_4 = df_1_index.join(df_2_index, on='mukey', how='left', lsuffix='_left', rsuffix='left')

df_3

df_ts['Date']

df_ts = df_ts[9748:9846] #only getting the time-series of at the dates we have obtained tweets (2019-08-10 to 2019-12-30)

df_ts

import datetime

def str_to_datetime(s):
  split = s.split('-')
  year, month, day = int(split[0]), int(split[1]), int(split[2])
  return datetime.datetime(year=year, month=month, day=day)

df_ts['Date'] = df_ts['Date'].apply(str_to_datetime)
df_ts = df_ts.set_index('Date')

from google.colab import drive
drive.mount('/content/drive')

df_both = df_final_weighted.join(df_ts)

df_both = df_both.dropna()

df_both

df_both.to_csv('WEIGHTED_FINAL')

df_both = pd.read_csv('WEIGHTED', engine ='python')

df_both['Date'] = df_both['Date'].apply(str_to_datetime)
df_both = df_both.set_index('Date')

df_both

from sklearn.preprocessing import minmax_scale

df_both['Adj Close']= minmax_scale(df_both['Adj Close'])

df_ts['Adj Close']= minmax_scale(df_ts['Adj Close'])

df_ts

# data frame creation: combining time series and sentiment analysis

from re import I
import numpy as np
from sklearn.preprocessing import minmax_scale

n=5 #number of predictive days
b=4 # lag between the predictive days and predicted day

def df_to_windowed_df(dataframe, first_date_str, last_date_str, n=n):
  first_date = str_to_datetime(first_date_str) #define the first date of the dataset
  last_date  = str_to_datetime(last_date_str) #define the last date of the dataset

  target_date = first_date #define the target date

  dates = [] #creating an empty list of dates
  #X, Y, W, Z, V = [], [], [], [], [] # creating 5 empty lists
  X, Y= [], []
  last_time = False

# creating an error in case the window size is too large for the target date
# making sure it iterates over n+1 rows at a time, n for predictions and 1 as a target

  while True:
    df_both = dataframe.loc[:target_date].tail(n+b)

    if len(df_both) != n+b:
      print(f'Error: Window of size {n} is too large for date {target_date}') # handling errors in case the data shape is not correct
      return

#df_subset is always made of n+1 rows

    #positive_mean = df_both['Weighted_Postitive'].to_numpy()
    #negative_mean = df_both['Weighted_Negative'].to_numpy()


    #w_positive = positive_mean[:-1].mean()
    #z_negative = negative_mean[:-1].mean()

    values = df_both['Adj Close'].to_numpy() # creating the variable 'values' that is made of the close price values of each df_subset rows (each dates), we converted it into a numpy array for handling.
    x, y = values[:-b], values[-1] # creating the variables 'x' and 'y' where 'x' is made of the timeseries of the n days prior to the predicted date and 'y' is made of the timeserie of the target date.

    dates.append(target_date) #appending the target dates to the list 'dates'
    X.append(x) #appending the timeseries values of the 'n' timeseries that will be used to predict the
    Y.append(y) #appending the times series of the target dates

    #W.append(w_positive)
    #Z.append(z_negative)

    next_week = dataframe.loc[target_date:target_date+datetime.timedelta(days=7)]
    next_datetime_str = str(next_week.head(2).tail(1).index.values[0])
    next_date_str = next_datetime_str.split('T')[0]
    year_month_day = next_date_str.split('-')
    year, month, day = year_month_day
    next_date = datetime.datetime(day=int(day), month=int(month), year=int(year))

    if last_time:
      break

    target_date = next_date

    if target_date == last_date:
      last_time = True

  ret_df = pd.DataFrame({})

  ret_df['Target Date'] = dates

  X = np.array(X)
  #W = np.array(W)
  #Z = np.array(Z)

  for i in range(0, n):
    X[:, i]
    #W[:]
    #Z[:]
    ret_df[f'Target-{n-i}'] = X[:, i]


  #ret_df[f'WEIGHTED_POS_MEAN_LAST_{n}_days'] = W[:]
  #ret_df[f'WEIGHTED_NEG_MEAN_LAST_{n}_days'] = Z[:]
  ret_df['Target'] = Y


  return ret_df

# time window
windowed_df = df_to_windowed_df(df_ts,
                                '2004-01-01',
                                '2019-12-30',
                                n=n)
windowed_df

from sklearn import preprocessing


windowed_df.values
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(x)
windowed_df = pd.DataFrame(x_scaled)

"""Converting our dataframe into a numpy array"""

from sklearn.preprocessing import minmax_scale


def windowed_df_to_date_X_y(windowed_dataframe):
  df_as_np = windowed_dataframe.to_numpy()

  dates = df_as_np[:, 0] # 0 just the first column, just to get the dates

  middle_matrix = df_as_np[:, 1:-1] #to get the matrix, we only want the second columns which is 1 and then the other columns, but not the last one -1

  # we need to reshape so it fits a LSTM model
  X = middle_matrix.reshape((len(dates), middle_matrix.shape[1], 1))
  #the first dimension is the lenght of dates, then the second piece is the middle matrix.shape sub 1, the last piece because we are only using 1 varialbe technically (), we have 3 values of that variables and how it changes over time, but it is one and only 1 variable. univariate forecasting = how the closing value is changing over time, if we used the open, high low and other varialbes, we woul dhvae to put another number there.
  # output vector, we want all the rows but not the last column.
  Y = df_as_np[:, -1]

  return dates, X.astype(np.float32), Y.astype(np.float32)



dates, X, y = windowed_df_to_date_X_y(windowed_df)


dates.shape, X.shape, y.shape # the data shape is important as it will define our numer of LSTM inputs. In this case: 757 timesteps and 1 feature, LSTM input shape will be (252,1)

"""Spliting the data into training, validating and testing data"""

import matplotlib.pyplot as plt

tr = 0.7 #percentage of training data
va = 0.8 # percentage of validation data


q_80 = int(len(dates) * tr) #these are just integers = 80% of the number of rows
q_90 = int(len(dates) * va) # 90% of the rows

dates_train, X_train, y_train = dates[:q_80], X[:q_80], y[:q_80] #dates training only until 80% of the dataset

dates_val, X_val, y_val = dates[q_80:q_90], X[q_80:q_90], y[q_80:q_90] #dates validating only between 80% to 90% of the dataset

dates_test, X_test, y_test = dates[q_90:], X[q_90:], y[q_90:] #dates testing only the last 10% of the dataset

plt.plot(dates_train, y_train, color='blue') #visualising the graph of the training dates on the x_axis in relation to the training timeseries values on the y_axis

plt.plot(dates_val, y_val, color='orange') #visualising the graph of the validating dates on the x_axis in relation to the validating timeseries values on the y_axis

plt.plot(dates_test, y_test, color='green') #visualising the graph of the testing dates on the x_axis in relation to the testing timeseries values on the y_axis

plt.legend(['Train', 'Validation', 'Test']) #ploting the graph

plt.rcParams["figure.figsize"] = (20,20)

!pip install bayesian-optimization
!pip install nimblenet
!pip install ml_metrics

"""Importing required modules for Bayesian optimisation of the hyperparameters and for the LSTM model"""

import numpy as np #importing numpy
import pandas as pd #importing pandas
import matplotlib.pyplot as plt #importing matplotlib for visualisation
import os
import tensorflow as tf

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #getting rid of tensorflow warnings

# importing sklearn modules:
from sklearn.model_selection import train_test_split #importing module for splitting training and testing data
from sklearn.model_selection import cross_val_score #importing cross validation
from sklearn.metrics import make_scorer
from sklearn.model_selection import KFold # importing KFold
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_squared_log_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_absolute_percentage_error

from ml_metrics import rmse
tf.keras.metrics.RootMeanSquaredError(
    name='root_mean_squared_error', dtype=None
)
tf.keras.metrics.MeanAbsoluteError(
    name='mean_absolute_error', dtype=None
)
tf.keras.metrics.MeanAbsolutePercentageError(
    name='mean_absolute_percentage_error', dtype=None
)
tf.keras.metrics.MeanSquaredError(
    name='mean_squared_error', dtype=None
)

#importing keras modules:

from keras.layers import LeakyReLU #importing activation function LeakyReLu
LeakyReLU = LeakyReLU(alpha=0.1)
from keras.wrappers.scikit_learn import KerasRegressor # importing KerasRegressor
from tensorflow.keras import layers #importing layers
from tensorflow.keras.models import Sequential  #importing Sequential for building the Neural Network
from keras.layers import Dense, BatchNormalization, Dropout #importing neural netowork parameters
from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl #importing different activation functions for testing
from keras.callbacks import EarlyStopping, ModelCheckpoint #importing EarlyStopping for computational efficieny (stops iterations when the metric function stops decreasing in our case)
from keras.layers.recurrent import LSTM


#importing Bayesian optimisation modules:
from bayes_opt import BayesianOptimization #importing the Bayesian optimisation module for hyperparameters value optimisation
#@Misc{,
    #author = {Fernando Nogueira},
    #title = {{Bayesian Optimization}: Open source constrained global optimization tool for {Python}},
    #year = {2014--},
    #url = " https://github.com/fmfn/BayesianOptimization"
#}

#import other modules
import warnings
warnings.filterwarnings('ignore')
pd.set_option("display.max_columns", None)
import math
from math import floor

from nimblenet.activation_functions import elliot_function

from scipy.special import softmax
import math
from tensorflow.keras import activations
from sklearn import linear_model

"""Creating a score function in order to define the best hyperparameters



"""

def nn_cl_bo(neurons, learning_rate, batch_size, epochs, dropout, alpha):


    alpha=alpha
    neurons = round(neurons) #rounding the number of neurons because we can not use fractions of neurons, because the LSTM model only accept integers and not floats
    batch_size = round(batch_size) #rouding the batch size to get a plain number of samples
    epochs = round(epochs)#rounding the number of epochs
    dropout=dropout


    def nn_cl_fun():
        model = Sequential() # as suggested by the Sequential documentation: 'A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor' (fchollet, 2020).
        model.add(LSTM(neurons, dropout=dropout, activation = tf.keras.layers.LeakyReLU(alpha=alpha), input_shape=(X.shape[1], 1))) #creating the the layer: an LSTM. Units=1 because all nodes the dimensionality of the space is =1 i.e.: all the input nodes are connected to only 1 output node. Also, we will iterate over the activation functions mentioned above. Also, the input_shape is set appropriately.
        model.add(Dense(neurons, input_dim=n)) #creating the first Dense layer, also iterating over the activation functions mentioned above. Input_dim = 3 as it takes 3 elements as inputs (the 3 time series of the 3 days prior to our target timeserie).
        model.add(Dense(1)) # Finally, a Dense layer as an output layer made of 1 neuron, also it iterates over the activation functions mentioned above.

        model.compile(loss='mean_absolute_error', optimizer=Adam(learning_rate = learning_rate), metrics='mean_absolute_error') #defining the loss function (useful to updates the weights), defining the meterics that measures the performance of all different combinations of models

        return model

    scorer = make_scorer(mean_absolute_error, greater_is_better=False)

    es = EarlyStopping(monitor='mean_absolute_error', mode='auto', verbose=0, patience=20) # This is used to monitor our metric in order to stop the training when no improvement is observed. No improvement after 20 epochs triggers the early stop. In 'min' mode, training will stop when the quantity monitored has stopped decreasing, as in this case the quantitiy monitored is an error function, it should stop when the error stops decreasing hence 'min'

    model = KerasRegressor(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size,
                         verbose=0)

    score = cross_val_score(model, X_train, y_train, scoring=scorer, cv=None, fit_params={'callbacks':[es]}).mean()

    return score #obtaining the score for each model with different combinations of hyperparaters

"""Running the Bayesian Optimisation"""

params_nn ={
    'neurons': (10, 100),
    'learning_rate':(0.00000001, 0.2),
    'batch_size':(32, 128),
    'epochs':(50, 250),
    'dropout':(0,0.3),
    'alpha': (0.00000001, 0.3)
}
# Running Bayesian Optimisation
nn_bo = BayesianOptimization(nn_cl_bo, params_nn, verbose =2, random_state=111) #calling the function that iterates over all our hyperparameters while measuring the performance of the underlyings LSTMs. Verbose 2 prints all trials
nn_bo.maximize(init_points=15, n_iter=15) #how many points between the range of values in the search space it will iterate over and the number of iterations

"""Printing the optimal hyperparameters"""

params_nn_ = nn_bo.max['params']


print("According to the Bayesian optimisation algorithm, the optimal parameters of our model are as follow:")
print("-------------------------------------------------------------------")

alpha_opt = list(params_nn_.values())[0]
params_nn_['neurons'] = round(params_nn_['neurons'])
params_nn_['batch_size'] = round(params_nn_['batch_size'])
params_nn_['epochs'] = round(params_nn_['epochs'])
params_nn_['learning_rate'] = params_nn_['learning_rate']
params_nn_['dropout'] = params_nn_['dropout']

print('the optimal alpha value of the LeakyRelu activation function of the LSTM is', alpha_opt)
print('the optimal number of neurons is', params_nn_['neurons'])
print('the optimal number of batch size is', params_nn_['batch_size'])
print('the optimal number of epochs is', params_nn_['epochs'])
print('the optimal learning rate is', params_nn_['learning_rate'])
print('the optimal dropout value is', params_nn_['dropout'])

# performing predictions with the optimised hyperparameters






model_opt = Sequential([layers.Input((5, 1)), #the shape is n as n predictive days + 2 which are the 2 values postitive mean of the n days and negative mean of the n days, 1 stands for univariate forecasting
                    layers.LSTM(params_nn_['neurons'], dropout=params_nn_['dropout'], activation = tf.keras.layers.LeakyReLU(alpha=alpha_opt), recurrent_activation='sigmoid',  input_shape=(X.shape[1], 1)),
                    layers.Dense(params_nn_['neurons']),
                    layers.Dense(1)]) # we only want to forecast 1 variables

model_opt.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=params_nn_['learning_rate']), metrics=['mean_squared_error'])

model_opt.fit(X_train, y_train, validation_data = (X_val, y_val), epochs=params_nn_['epochs'], batch_size=params_nn_['batch_size'], verbose=2)

# MSE - SQUARED
score_training = model_opt.evaluate(X_train, y_train)
score_val = model_opt.evaluate(X_val, y_val)
score_test = model_opt.evaluate(X_test, y_test)

print('---------------')

print('Training loss:', score_training[0])
print('Training mean_squared_error:', score_training[1])

print('---------------')

print('Validation loss:', score_val[0])
print('Validation mean_squared_error:', score_val[1])

print('---------------')

print('Test loss:', score_test[0])
print('Test mean_squared_error:', score_test[1])

print('---------------')

# plotting the graphs

train_predictions = model_opt.predict(X_train).flatten()

plt.plot(dates_train, train_predictions, color='blue')
plt.plot(dates_train, y_train, color='red')
plt.legend(['Training Predictions', 'Training Observations'])

plt.rcParams["figure.figsize"] = (15,10)

val_predictions = model_opt.predict(X_val).flatten() # this is where the model tries to predict validation data (10%)

plt.plot(dates_val, val_predictions, color='blue')
plt.plot(dates_val, y_val, color='red')
plt.legend(['Validation Predictions', 'Validation Observations'])

plt.rcParams["figure.figsize"] = (15,10)


#exacly the same as the train

# this is where the model tries to predict the test data (10%)
test_predictions = model_opt.predict(X_test).flatten()

plt.plot(dates_test, test_predictions, color='blue')
plt.plot(dates_test, y_test, color='red')
plt.legend(['Testing Predictions', 'Testing Observations'])

plt.rcParams["figure.figsize"] = (15,10)


#test prediction, same as training and validation

#LSTM are very bad at extrapolating, it has no idea what to do with test = learn data outside its range. Another way to think aboutt it. Might not be that helpful, maybe we don't want to train it on all of it (in the tutorial, from 1986 to 2024) we maybe only want to train it lately. only 1 year. We need to change the window function above = how we call it.
# it is good at predicting values in teh same range as it has bee trained, we only change the numbers in teh model

# how to predict long term? we use those 3 days before to make those predictions. We train here and model recursively predict the future.

# basiclaly we are going to only use the training data, the model has already been trained on those data.
# looping through those data and get
# prediction for the next day


from copy import deepcopy

recursive_predictions = []
recursive_dates = np.concatenate([dates_val, dates_test])

# we concatenate the validation and test dates
# we can loop through those dates

# on the long term = completely useless, but useful on the micro scale, per day, should I buy or not.


for target_date in recursive_dates:
  last_window = deepcopy(X_train[-1])
  next_prediction = model_opt.predict(np.array([last_window])).flatten() #for the next day
  recursive_predictions.append(next_prediction)
  last_window[-1] = next_prediction

plt.plot(dates_train, train_predictions)
plt.plot(dates_train, y_train)
plt.plot(dates_val, val_predictions)
plt.plot(dates_val, y_val)
plt.plot(dates_test, test_predictions)
plt.plot(dates_test, y_test)


plt.plot(recursive_dates, recursive_predictions)
plt.legend(['Training Predictions',
            'Training Observations',
            'Validation Predictions',
            'Validation Observations',
            'Testing Predictions',
            'Testing Observations',
            'Recursive Predictions'])

"""MOST IMPORTANT HYPERPARAMETERS"""

import pandas as pd
import numpy as np
df = pd.read_csv('MOST INFLUENCIAL 2_2.csv', encoding = 'unicode_escape', engine ='python')
df

"""LOG"""

df['TARGET_LOG'] = np.log(df['Target'])
df['ALPHA_LOG'] = np.log(df['Alpha'])
df['BATCH_LOG'] = np.log(df['Batch'])
df['DROP_LOG'] = np.log(df['Dropout'])
df['EPOCHS_LOG'] = np.log(df['Epochs'])
df['LEARNING_LOG'] = np.log(df['Learning'])
df['NEURONS_LOG'] = np.log(df['Neurons'])

df_log = df.filter(['TARGET_LOG', 'ALPHA_LOG', 'BATCH_LOG', 'DROP_LOG', 'EPOCHS_LOG', 'LEARNING_LOG', 'NEURONS_LOG'])

df_log

import matplotlib.pyplot as plt


corr=df_log.corr()#givesusthecorrelationvalues
plt.figure(figsize=(15,10))

X_LOG=df_log.drop(['TARGET_LOG'],axis=1)
y_LOG=df_log['TARGET_LOG']
df_log.head()
df_=df_log.apply(pd.to_numeric,errors='coerce')
df_log=df_log.dropna(axis=0)

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

kbest = SelectKBest(score_func=f_regression, k='all')

ordered_features = kbest.fit(X_LOG, y_LOG)

df_scores=pd.DataFrame(ordered_features.scores_, columns=['f-value'])#savethescoresintoadataFrame

df_b=pd.DataFrame(ordered_features.pvalues_, columns=['p-value'])#savethescoresintoadataFrame

df_columns=pd.DataFrame(X_LOG.columns, columns=['Hyperparameters'])#savethefeaturenamesofthosescoresinto

feature_rank=pd.concat([df_scores, df_b, df_columns],axis=1)#combinethetwodataFrames

feature_rank.nlargest(6,'f-value') #rank the features by score­ the scores are based on the f-regressor scoring funct

df_toprint=

from sklearn.ensemble import ExtraTreesRegressor #this is a method alternative to k­best

 #ExtraTreesClassifer:METHOD 2 USED TO SCORE AND RANK THE MOST CORRELATED FEATURE AGAINST THE TARGET.
model=ExtraTreesRegressor()
model.fit(X_LOG,y_LOG)#pass the input and output data to the method


model.feature_importances_#compute the scores

ranked_features=pd.Series(model.feature_importances_,index=X_LOG.columns)#put the scores in a series

ranked_features.nlargest(6).plot(kind='bar')#rank and plot the scores

#MUTUAL INFO CLASSIFIER:METHOD 3 USED TO SCORE AND RANK THE MOST CORRELATED FEATUREA GAINST THE TARGET.

from sklearn.feature_selection import mutual_info_regression
mu_ifo=mutual_info_regression(X_LOG,y_LOG)

mu_data=pd.Series(mu_ifo,index=X_LOG.columns)
mu_data.sort_values(ascending=False)



"""    MINMAX"""

from sklearn.preprocessing import minmax_scale

from sklearn import preprocessing

scaler = preprocessing.MinMaxScaler()
min_max_scaler = preprocessing.MinMaxScaler()

def scaleColumns(df, cols_to_scale):
    for col in cols_to_scale:
        df[col] = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(df[col])),columns=[col])
    return df

scaleColumns(df,['Batch', 'Dropout', 'Epochs', 'Neurons'])
df

df_minmax = df.filter(['Target', 'Alpha', 'Batch', 'Dropout', 'Epochs', 'Learning', 'Neurons'])

df_minmax

corr=df_minmax.corr()#givesusthecorrelationvalues
 plt.figure(figsize=(15,10))
 sns.heatmap(corr,annot=True,cmap="BuPu")

X_OK=df_minmax.drop(['Target'],axis=1)
y_OK=df_minmax['Target']
df_minmax.head()
df_minmax=df_minmax.apply(pd.to_numeric,errors='coerce')
df_minmax=df_minmax.dropna(axis=0)

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression

kbest = SelectKBest(score_func=f_regression, k='all')
ordered_features = kbest.fit(X_OK,y_OK)


df_scores=pd.DataFrame(ordered_features.scores_,columns=['SCORE'])#savethescoresintoadataFrame

df_columns=pd.DataFrame(X_OK.columns,columns=['para_name'])#savethefeaturenamesofthosescoresinto

feature_rank=pd.concat([df_scores,df_columns],axis=1)#combinethetwodataFrames

feature_rank.nlargest(6,'SCORE') #rank the features by score­ the scores are based on the f-regressor scoring funct

from sklearn.ensemble import ExtraTreesRegressor #this is a method alternative to k­best

 #ExtraTreesClassifer:METHOD 2 USED TO SCORE AND RANK THE MOST CORRELATED FEATURE AGAINST THE TARGET.
model=ExtraTreesRegressor()
model.fit(X_OK,y_OK)#pass the input and output data to the method


model.feature_importances_#compute the scores

ranked_features=pd.Series(model.feature_importances_,index=X_OK.columns)#put the scores in a series

ranked_features.nlargest(6).plot(kind='bar')#rank and plot the scores

#MUTUAL INFO CLASSIFIER:METHOD 3 USED TO SCORE AND RANK THE MOST CORRELATED FEATUREA GAINST THE TARGET.

from sklearn.feature_selection import mutual_info_regression
mu_ifo=mutual_info_regression(X_OK,y_OK)

mu_data=pd.Series(mu_ifo,index=X_OK.columns)
mu_data.sort_values(ascending=False)